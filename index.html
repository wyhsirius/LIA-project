<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="description" content="Latent Image Animator: Learning to Animate Images via Latent Space Navigation">
    	<meta name="author" content="STARS team">

    	<title>LIA</title>
		
    	<link href="css/bootstrap.min.css" rel="stylesheet">
    	<link href="css/style.css" rel="stylesheet">
    </head>

    <body>

<div class="container">
      <div class="header">
        <h2><center>Latent Image Animator: Learning to Animate Images via Latent Space Navigation</center> </h2>
		<h4><center>In ICLR 2022</center></h4>
		<h4> <center> <a href="http://www-sop.inria.fr/members/Yaohui.Wang" class="text-info"> Yaohui Wang </a> <a href="" class="text-info">&nbsp&nbsp&nbsp&nbsp Di Yang</a> &nbsp&nbsp&nbsp&nbsp <a href="http://www-sop.inria.fr/members/Francois.Bremond/" class="text-info">François Brémond</a> &nbsp&nbsp&nbsp&nbsp <a href="http://antitza.com" class="text-info">Antitza Dantcheva</a> </center> </h4>
      	<h4> <center> Inria, Université Côte d&#39Azur</center> </h4>
	  </div>

      <center>
	      <img src="g3animgs/g3an.png" title="" style="max-width:80%;vertical-align:top"/>
		  <h3>
		  	<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_G3AN_Disentangling_Appearance_and_Motion_for_Video_Generation_CVPR_2020_paper.pdf" class="text-info">[PDF]</a>
	        <a href="https://github.com/wyhsirius/g3an-project" class="text-info">[Code]</a>
		  	<a href='g3an.txt' class="text-info">[Bibtex]</a>
	      </h3>
      	
	  </center>
	
      <div class="row">
        <h2>Abstract</h2>
	  <center>
        <p style="text-align: justify;">
Creating realistic human videos introduces the challenge of being able to simultaneously generate both appearance, as well as motion.
To tackle this challenge, we introduce G&sup3AN, a novel spatio-temporal generative model, which seeks to capture the distribution of high dimensional video data and to model appearance and motion in disentangled manner.
The latter is achieved by decomposing appearance and motion in a three-stream Generator, where the main stream aims to model spatio-temporal consistency, whereas the two auxiliary streams augment the main stream with multi-scale appearance and motion features, respectively.
An extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as well as the Weizmann and UCF101 datasets on human action.
Additional analysis on the learned latent representations confirms the successful decomposition of appearance and motion.</p>
	 </center>
	 </div>
	
		
	<div class="row">
    <h2> Demos </h2>
	<center>
    <img src="demos/uncond.gif"  width='560' height='360' />
	<img src="demos/cond.gif"  width='560' height='360' />
    <figcaption><h4> Unconditional & Conditional Generation</h4></figcaption>
	
	<img src="demos/multilength1.gif" width='560' height='360' />
	<img src="demos/multilength2.gif" width='560' height='360' />
		<figcaption><h4> Multi-length Generation </h4></figcaption>
	</center>
	</div>	

     <div class="row">
        <h2>Acknowledgements</h2>
        <center>
		<p style="text-align: justify;">This work is supported by the French Government (National Research Agency, ANR), under Grant ANR-17-CE39-0002.<p>
		</center>
    </div> 
		
     <div class="row">
        <h2>Copyright Notice</h2>
        <center>
		<p style="text-align: justify;">The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author’s copyright.<p>
		</center>
    </div> 	
	
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    </body>
</html>
